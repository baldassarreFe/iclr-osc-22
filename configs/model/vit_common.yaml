backbone:
  patch_size: [8, 8]
  # Embedding dimension C
  embed_dim: 64
  # Positional embedding
  pos_embed: "learned"
  pos_embed_dropout: 0.0
  pos_embed_every_layer: yes
  # Number of layers L
  num_layers: 4  # L
  # Number of heads h
  num_heads: 4
  # Block configuration
  block_drop: 0.0
  block_attn_drop: 0.0
  drop_path: 0.0
  mlp_ratio: 2.0

global_fn:
  pooling: 'avg'
  hidden_mult: 2.0
  activation: 'gelu'
  dropout: 0.0

global_proj:
  hidden_mult: 2.0
  activation: 'gelu'
  dropout: 0.0
  out_bias: no

obj_fn:
  name: 'slot-attention'
  hidden_mult: 2.0
  # Number of query tokens S
  num_objects: 11
  queries: "sample"
  pos_embed: null
  num_iters: 3

obj_proj:
  hidden_mult: 2.0
  activation: 'gelu'
  dropout: 0.0
  out_bias: no
