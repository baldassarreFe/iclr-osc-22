# Enable with model/backbone=vit_tiny_patch16_224_in21k

name: 'vit'

# Input
patch_size: [16, 16]

# Embedding dimension C
embed_dim: 192

# Positional embedding
pos_embed_drop: 0.0

# Block configuration
num_layers: 12
num_heads: 3
proj_drop: 0.0
attn_drop: 0.0
drop_path: 0.0
mlp_ratio: 4.0
qkv_bias: True

# Output (pretrained model has LayerNorm before the output)
output_norm: false

# Pretrained config from timm.models.vision_transformer
pretrained: vit_tiny_patch16_224_in21k
frozen: true
