# Don't use this config directly, include one of the two architectures
# from the command line, e.g. model=vit_global_obj, and select the
# obj_fn as model/obj_fn=slot_attention

architecture: null

backbone:
  patch_size: [8, 8]
  # Embedding dimension C
  embed_dim: 64
  # Positional embedding
  pos_embed: "learned"
  pos_embed_dropout: 0.0
  pos_embed_every_layer: true
  # Number of layers L
  num_layers: 4  # L
  # Number of heads h
  num_heads: 4
  # Block configuration
  block_drop: 0.0
  block_attn_drop: 0.0
  drop_path: 0.0
  mlp_ratio: 2.0
  # Outputs
  global_pool: 'cls'

global_fn:
  hidden_mult: 2.0
  activation: 'gelu'
  dropout: 0.0

global_proj:
  hidden_mult: 2.0
  activation: 'gelu'
  dropout: 0.0
  out_bias: false

obj_queries:
  name: "sample"
  num_objects: 11 # S

# Specified in its own config group
obj_fn: null

obj_proj:
  hidden_mult: 2.0
  activation: 'gelu'
  dropout: 0.0
  out_bias: false
